    \documentclass[12pt,final,fleqn]{article}

% basic packages
\usepackage[margin=1in] { geometry }
\usepackage{amssymb,amsmath, bm}
\usepackage{verbatim}
\usepackage[latin1]{inputenc}
%\usepackage[OT1]{fontenc}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage[bottom]{footmisc}
\usepackage{url}
\usepackage[font={bf}]{caption}
\usepackage{float}
%\usepackage{pgfplots}
%\usepackage[font={bf}]{caption}
\usepackage{setspace}
\usepackage{latexsym}
%\usepackage{euscript}
\usepackage{graphicx}
\usepackage{marvosym}
\usepackage{amsmath} 
\usepackage{authblk}
\usepackage{xcolor}
\usepackage{blindtext}
%\usepackage[varg]{txfonts}  Older version of ``g'' in math.

% bibliography packages
\usepackage[natbibapa]{apacite}
\bibliographystyle{apacite}
\bibpunct{(}{)}{;}{a}{}{,}
\renewcommand{\bibname}{References}
\defcitealias{lokniti2014}{CSDS 2014}

% hyperref options
\usepackage{color}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={blue!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\newcommand*{\Appendixautorefname}{Appendix}
\renewcommand*{\sectionautorefname}{Section}
\renewcommand*{\subsectionautorefname}{Section}
\newcommand{\aref}[1]{\hyperref[#1]{Appendix~\ref{#1}}}

% packages for tables
\usepackage{longtable}
\usepackage{booktabs, threeparttable}
\usepackage{threeparttablex}
%\usepackage{tabularx}
% dcolumn package
\usepackage{dcolumn}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\captionsetup{belowskip=10pt,aboveskip=-5pt}
\usepackage{multirow}
% rotating package
\usepackage[figuresright]{rotating}
\usepackage{pdflscape}
\usepackage{subcaption}

% packages for figures
\usepackage{grffile}
\usepackage{afterpage}
\usepackage{float}
\usepackage[section]{placeins}


% theorem package
\usepackage{theorem}
\theoremstyle{plain}
\theoremheaderfont{\scshape}
\newtheorem{theorem}{Theorem}
\newtheorem{algorithm}{Algorithm}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newcommand{\qed}{\hfill \ensuremath{\Box}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\argmin}{arg\min}
\DeclareMathOperator{\argmax}{arg\max}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\providecommand{\norm}[1]{\lVert#1\rVert}
\renewcommand\r{\right}
\renewcommand\l{\left}
\newcommand\E{\mathbb{E}}
\newcommand\dist{\buildrel\rm d\over\sim}
\newcommand\iid{\stackrel{\rm i.i.d.}{\sim}}
\newcommand\ind{\stackrel{\rm indep.}{\sim}}
\newcommand\cov{{\rm Cov}}
\newcommand\var{{\rm Var}}
\newcommand\SD{{\rm SD}}
\newcommand\bone{\mathbf{1}}
\newcommand\bzero{\mathbf{0}}

% dotted lines in tables
%\usepackage{arydshln}

\usepackage{pdflscape}

% spacing between sections and subsections
\usepackage[compact]{titlesec}

% times new roman
%\usepackage{times}

% appendix settings
\usepackage[toc,page,header]{appendix}
\renewcommand{\appendixpagename}{\centering Appendices}
\usepackage{chngcntr}
\usepackage{etoolbox}
\usepackage{lipsum}


% file paths and definitions
\makeatletter
\newcommand*\ExpandableInput[1]{\@@input#1 }
\makeatother

\setlength{\mathindent}{1cm}
\allowdisplaybreaks[4]
\doublespacing
%\special{pdf: pagesize width 8.5truein height 11.0truein}

\titleformat{\subsection}
  {\itshape\large}{\thesubsection}{1em}{}

\setcounter{tocdepth}{1}

%--------------------------------------------------------------------------------------
% BEGIN DOCUMENT
%--------------------------------------------------------------------------------------

\begin{document}
\singlespace
\title{\textbf{Corruption information and vote share: A meta-analysis and lessons for survey experiments}\vspace{-1ex}\thanks{}}
%I would like to thank Alex Coppock, Angele Delevoye, Devin Incerti, Josh Kalla, Dan Mattingly, Susan Rose-Ackerman, Frances Rosenbluth, Radha Sarkar, and Fredrik Sävje for exceptionally helpful feedback and suggestions. Any and all errors are my own.

\author{Trevor Incerti\thanks{trevor.incerti@yale.edu}\vspace{-1ex}}
\affil{\textit{Yale University}\vspace{-2.5ex}}
\date{\today}
\maketitle

\begin{abstract}
\noindent
Do voters in democratic countries hold politicians accountable for corruption? Field experiments that provide voters with information about the corrupt acts of politicians then monitor vote choice have become standard in political science and economics. Similarly, vote choice survey experiments commonly provide respondents with information about the corrupt acts of hypothetical candidates. What have we learned from these experiments? A meta-analysis demonstrates that the aggregate treatment effect of providing information about corruption on vote share in field experiments is approximately zero. By contrast, corrupt candidates are punished by respondents by approximately 33-35 percentage points across survey experiments. This suggests that while vote-choice survey experiments may provide information on the directionality of informational treatments in idealized hypothetical scenarios, the point estimates they provide may not be representative of real-world voting behavior.
\end{abstract}

\doublespace

\begin{center}
First draft: March 2019. This draft: May 2019 \\~\\

\textcolor{red}{[PRELIMINARY DRAFT: ADDITIONAL STUDIES TO BE ADDED AND POINT ESTIMATES REFINED. PLEASE DO NOT CITE OR CIRCULATE WITHOUT AUTHOR'S PERMISSION.]}
\end{center}

\pagebreak

\section{Introduction} \label{sec:Introduction}

Competitive elections create a system whereby voters can hold policy makers accountable for their actions. This mechanism should make politicians hesitant to engage in malfeasance such as blatant acts of corruption. Increases in public information regarding corruption should therefore decrease levels of corruption in government, as voters armed with information should expel corrupt politicians \citep{gray1998corruption, kolstad2009transparency, rose2016corruption}. However, this theoretical prediction is undermined by the observation that well-informed voters continue to vote corrupt politicians into office in many democratic states. Political scientists and economists have therefore turned to experimental methods to test the causal effect of learning about politician corruption on vote choice.

Numerous experiments have examined whether providing voters with information about the corrupt acts of politicians decreases their re-election rates. These papers  often suggest that there is little consensus on how voters respond to information about corrupt politicians \citep{botero2015says, buntaine2018sms, arias2018priors, klavsnja2017voters, solaz2018group, de2017electoral}. Others indicate that experiments have provided us with evidence that voters strongly punish individual politicians involved in malfeasance \citep{chong2014does, winters2015political, winters2016s, weitz2017can}.

By contrast, this meta-analysis suggests that: (1) In aggregate, the effect of providing information about incumbent corruption on incumbent vote share in field experiments is approximately zero, and (2) corrupt candidates are punished by respondents by approximately 33-35 percentage points across survey experiments. I also examine the mechanisms that may give rise to this discrepancy. I find no evidence of publication bias, implying that the discrepancy arises due to differences in experimental design. Social desirability bias may lead survey experiments to capture anti-corruption norms rather than realistic voter behavior, and the same norms may lead voters to select a clean candidate in a simple hypothetical vignette where respondents lack information. Conjoint experiments attempt to alleviate this issue, but are often analyzed in ways that may fail to illuminate the most substantively important comparisons.  

%A future draft of this paper explores the mechanisms behind these effects, testing for social desirability bias, publication bias, etc. 


\section{Corruption information and electoral accountability} \label{sec: information}

Experimental support for the hypothesis that providing voters with information about politicians' corrupt acts decreases their re-election rates is mixed. Field experiments have provided some causal evidence that informing voters of candidate corruption has negative (but generally small) effects on candidate vote-share. This information has been provided by: randomized financial audits \citep{ferraz2008exposing}, fliers revealing corrupt actions of politicians \citep{de2011voters, chong2014does}, and SMS messages \citep{buntaine2018sms}. However, near-zero and null findings are also prevalent, and the negative and significant effects reported above sometimes only manifest in particular subgroups. \citet{banerjee2010can} primed voters in rural India not to vote for corrupt candidates, and \citet{banerjee2011informed} provided information on politicians' spending discrepancies, with both studies finding near-zero and null effects on vote share. \citet{boas2018norms} similarly find zero and null effects from distributing fliers in Brazil. Finally, \citet{arias2018priors, arias2019information} find that providing Mexican voters with information (fliers) about mayoral corruption actually \textit{increased} incumbent party vote share by 3\%.\footnote{The authors theorize that this average effect stems from levels of reported malfeasance actually being lower than voters no-information expectations of corruption.} 

By contrast, survey experiments consistently show large negative effects from information treatments on hypothetical vote share. These experiments often manipulate moderating factors other than information provision (e.g. quality of information, source of information, whether the candidate is a co-partisan or co-ethnic, whether corruption brings economic benefits, etc.), but even so systematically show negative treatment effects \citep{avenburg2016corruption, anduiza2013turning, banerjee2014poor, breitenstein2019choosing, boas2018norms, eggers2018corruption, franchino2015voting, klavsnja2013economy, klavsnja2017voters, mares2019voting, vera2017heterogeneous, winters2013lacking, winters2015political, winters2016s, weitz2017can, winters2018information}. These experiments have historically taken the form of single treatment arm or multiple arm factorial vignettes, but more recently have tended toward conjoint experiments \citep{mares2019voting, klavsnja2017voters, franchino2015voting, breitenstein2019choosing, chauchard2017limited}.

\citet{boas2018norms} find differential results that they obtain from a field and survey experiment---zero and null in field, large and negative in survey. They argue that this may reflect that norms against malfeasance in Brazil do not translate into action in real life. \citet{boas2018norms} point to features specific to Brazil in their explanation of this discrepancy, namely lower salience of corruption to Brazilian voters in municipal elections and the strong effects of dynastic politics. However, meta-analysis demonstrates that this is not only the case for \citet{boas2018norms}'s experiments in Brazil, but extends across a systematic review of all studies conducted to date. This suggests that such a discrepancy between field and survey experimental findings is driven by methodological differences, rather than context. 

Lab experiments that reveal corrupt actions to fellow players appear to be similar to survey experiments, and also show large negative treatment effects (see \autoref{fig: lab}). While there are not enough lab experiments examining whether the provision of corruption information impacts vote choice to conduct a formal meta-analysis \citep{arvate2017condemning, azfar2007transparency, solaz2018group, rundquist1977corrupt}, this discrepancy is worth noting as previous examinations of lab-field correspondence have found evidence of general replicability \citep{camerer2011promise, coppock2015assessing}.


\section{Moderating factors}\label{sec: moderators}

Even if voters generally find corruption distasteful, the quality of the information provided or positive candidate attributes and policies may outweigh the negative effects of corruption to voters, mitigating the effects of information provision on vote-share.\footnote{See \citet{de2017electoral} for a comprehensive overview.} These mitigating factors will naturally arise in a field setting, but may only be salient to respondents if specifically manipulated by researchers in a survey setting. A number of survey experiments have therefore added factors other than corruption as mitigating variables, some of which are described below. 

\subsection{Quality of information}

Corruption accusations can come from a variety of sources, some more credible than others. For example, accusations from an independent anti-corruption authority may be deemed more credible than those from an opposition party, and accusations may be deemed less credible than a conviction. Multiple studies have therefore attempted to randomize the quality of information provided to voters in order to capture how the electoral penalties vary in response to information quality \citep{botero2015says, breitenstein2019choosing, mares2019voting, banerjee2014poor, weitz2017can, winters2018information}. As expected, higher quality information produces larger negative treatment effects in these experiments (see \autoref{fig: quality}). 

\subsection{Policy stances}

Response to favorable policy stances has been shown to potentially mitigate the impact of corruption to voters. \citet{rundquist1977corrupt} use a survey experiment to show that a candidate's position on the Vietnam War could significantly increase the likelihood of voting for a ``corrupt'' candidate in the United States. \citet{franchino2015voting} examine corruption in relation to a candidate's education, income, tax policy, and same-sex marriage beliefs in Italy, and show that respondents prefer corrupt but socially and economically progressive candidates to clean but conservative candidates.

\subsection{Economic benefit}

Economic benefit has been argued to act as a similar mitigating factor.  \citet{klavsnja2017voters} randomize party, economic performance, and whether or not the politician's corrupt act itself brought benefits to their constituents in Argentina, Chile, and Uruguay, finding evidence that voters are more forgiving of corruption when it benefits them personally. By contrast, \citet{winters2013lacking} use a survey experiment in Brazil to show that voters punish corrupt politicians at the polls, including those with strong records of past performance as measured by public goods provision.

\subsection{Partisanship and in-group attachments}

Evidence of co-partisanship as a limiting factor to corruption deterrence is mixed. \citet{anduiza2013turning} and \citet{breitenstein2019choosing} both show that co-partisanship decreases the importance of corruption to Spanish voters using survey experiments. \citet{solaz2018group} induce in-group attachment in a lab-experiment of UK subjects, finding that in-group membership reduces sanction of ``corrupt'' participants. However, \citet{klavsnja2017voters} find relatively small effects of co-partisanship in Argentina, Chile, and Uruguay, and \citet{rundquist1977corrupt} find null effects in the US in the 1970s. \citet{konstantinidis2013sources} also find that partisanship does not moderate electoral punishment of corruption in a survey experiment in Greece. This evidence suggests that strong partisan effects occur where partisan attachments are strongest. Likewise, if co-ethnicity mitigates punishment of corrupt behavior, we may see these effects in highly fractionalized societies.

\section{Research Design and Methods} \label{sec: Methods}

\subsection{Selection criteria}\label{sec: criteria}

I followed standard practices to locate the experiments included in the meta-analysis. This included following citation chains and searches of data bases using the terms (``corruption experiment,'' ``corruption field experiment,'' ``corruption survey experiment,'' ``corruption factorial'', ``corruption candidate choice'', ``corruption conjoint'', ``corruption, vote, experiment'', and ``corruption vignette''). Papers from any discipline are eligible for inclusion, but in practice stem only from economics and political science. Both published articles and working papers are included so as to ensure the meta-analysis is not biased towards published results. In total, I located 10 field experiments from 8 papers, and 18 survey experiments from 15 papers.

Field experiments are included if researchers randomly assigned information regarding incumbent corruption to voters, then measured corresponding voting outcomes. This therefore excludes experiments that randomly assign corruption information, but use favorability ratings or other metrics rather than actual vote share as their dependent variable \citep{green2018publicizing}. I include one natural experiment, \citet{ferraz2008exposing}, as random assignment was conducted by the Brazilian government. Effects reported in the meta-analysis come from information treatments on the entire sample of study only, not subgroup or interactive effects that reveal the largest treatment effects.

For survey experiments, studies must test a no-information control group versus a corruption information treatment group and measure vote choice for a hypothetical candidate. This necessarily excludes studies that compare one type of information provision (e.g. source) to another and the control group is one type of information rather than no information, or where the politician is always known to be corrupt \citep{konstantinidis2013sources, botero2015says, anduiza2013turning, rundquist1977corrupt, munoz2012voters, weschle2016punishing}. In many cases, studies have multiple corruption treatments (e.g. high quality information vs. low quality information, co-partisan vs. opposition party, etc.). In these cases, I replicate the studies and code corruption as a binary treatment (0 = clean, 1 = corrupt) where \textit{all} treatment arms that provide corruption information are combined into a single treatment. Studies that use non-binary vote choices are rescaled into a binary vote choice.\footnote{For example, a 1-4 scale is recoded so that 1 or 2 is equal to no vote, and 3 or 4 is equal to a vote.} In some cases (5 total), point estimates, standard errors and/or confidence intervals are not explicitly reported, and standard errors are estimated by digitally measuring coefficient plots.\footnote{I recognize that this introduces non-statistical measurement error into the meta-analysis. However, it is not possible for these errors to be large enough to effect the substantive conclusions of the analysis.}

\subsection{Included studies}\label{sec: included}

A full list of all papers - disaggregated by field and survey experiments - that meet the criteria outlined above are provided in \autoref{tab:field} and \autoref{tab:survey}. A list of lab experiments (4 total) can also be found in and \autoref{tab:lab}, although these studies are not included in the meta-analysis. 

\begin{table}[!htbp] \centering 
  \caption{Field experiments}
  \label{tab:field} 
  \small
\begin{tabular}{@{\extracolsep{0pt}} cccccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 Study & Country & Treatment \\ 
\hline \\[-1.8ex] 
\citet{arias2018priors} & Mexico & Fliers \\
\citet{banerjee2010can} & India & Newspaper \\
\citet{banerjee2011informed} & India & Canvas/Newspaper \\
\citet{boas2018norms} & Brazil & Fliers \\
\citet{buntaine2018sms} & Ghana & SMS \\
\citet{chong2014does} & Mexico & Fliers \\
\citet{de2011voters} & Brazil & Fliers \\
\citet{ferraz2008exposing} & Brazil &  Audits \\ 
%\citet{humphreys2012policing} & Uganda & Fliers \\
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 
\FloatBarrier

\begin{table}[!htb] \centering 
  \caption{Survey experiments}
  \label{tab:survey} 
  \small
\begin{tabular}{@{\extracolsep{5pt}} cccccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 Study & Country & Type of survey \\ 
\hline \\[-1.8ex] 
\citet{avenburg2016corruption} & Brazil & Vignette  \\
%\citet{anduiza2013turning} & Spain & Information\\
\citet{banerjee2014poor} & India & Vignette \\
\citet{breitenstein2019choosing} & Spain & Conjoint \\
\citet{boas2018norms} & Brazil & Vignette \\
\citet{chauchard2017limited} & India & Conjoint \\
%\citet{de2011voters} & Brazil & Fliers \\
\citet{eggers2018corruption} & UK & Conjoint \\
\citet{franchino2015voting} & Italy & Conjoint \\
\citet{klavsnja2013economy} & Sweden & Vignette \\
\citet{klavsnja2013economy} & Moldova & Vignette \\
\citet{klavsnja2017voters} & Argentina & Conjoint \\
\citet{klavsnja2017voters} & Chile & Conjoint \\
\citet{klavsnja2017voters} & Uruguay & Conjoint \\
\citet{mares2019voting} & Romania & Conjoint \\
\citet{vera2017heterogeneous} & Peru & Vignette \\
\citet{winters2013lacking} & Brazil & Vignette \\
\citet{winters2015political} & Brazil & Vignette \\
\citet{winters2016s} & Brazil & Vignette \\
\citet{weitz2017can} & Brazil & Vignette \\
\citet{winters2018information} & Argentina & Vignette \\
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 
\FloatBarrier

\subsection{Additional selection comments}\label{sec: additional_comments}

Additional discussion of justification for the inclusion or exclusion of studies, as well as coding and/or replication choices may be warranted in some cases. 

The field experiment conducted by \citet{banerjee2010can} is included. However, the authors treated voters with a campaign not to vote for corrupt candidates, but did not provide voters with information on which candidates were corrupt. Similarly, the field experiment conducted by \citet{banerjee2011informed} is included, but their treatment provided information on politicians' spending discrepancies, which may imply corruption but is not as direct as other types of information provision. The results are not sensitive to the inclusion of these studies (see \autoref{fig: meta-field_no_banerjee}). The point estimates remain approximately zero percentage points using random effects estimation, and are approximately -1 percentage using fixed effects estimation. 

With respect to survey experiments, \citet{chauchard2017limited} include two treatments, wealth accumulation and whether the wealth accumulation was is illegal. The effect reported here is the illegal treatment only. This is likely a conservative estimate, as the true effect is a combination of illegality and wealth accumulation. \citet{winters2016s} and \citet{weitz2017can} report results from the same survey experiment. The results are therefore only reported once. The survey experiment in \citet{de2011voters} is excluded from the analysis as it does not use hypothetical candidates, but instead asks voters if they would have changed their actual voting behavior in response to receiving corruption information. This study has a null finding. The overall results are not sensitive to the inclusion of this estimate (see \autoref{fig: meta-field_defig}). 

\subsection{Results}\label{sec: results}

\begin{figure}[!htb]
\includegraphics{../figs/field.pdf}
\vspace{0.2cm}
\caption{Field experiments: Average treatment effect of corruption information on incumbent vote share}
\small
\vspace{-0.3cm}
\label{fig: meta-field}

\vspace{1.5cm}

\includegraphics{../figs/survey.pdf}
\vspace{0.2cm}
\caption{Survey experiments: Average treatment effect of corruption information on incumbent vote share}
\small
\vspace{-0.3cm}
\label{fig: meta-survey}
\end{figure}

Survey experiments demonstrate much larger negative treatment effects of providing information about corruption to voters relative to field experiments. In fact, the results in \autoref{fig: meta-field} reveal a point estimate of approximately zero and suggest that we cannot reject the null hypothesis of no treatment effect in field experiments.  While recognizing the power constraints inherent in such a small sample size, a univariate Shapiro-Wilk test of normality suggests that we cannot reject the null hypothesis that the point estimates are distributed normally around a mean of approximately zero percentage points. 

By contrast, corrupt candidates are punished by respondents by approximately 35 percentage points in survey experiments based on fixed effects meta-analysis and 33 percentage points using random effects meta-analysis. Of the 18 survey experiments, only one shows a null effect \citep{klavsnja2013economy}, while all others are negative and significantly different from zero at conventional levels. 

Examining all studies together, a test for heterogeneity by type of experiment (field or survey) reveals that up to 66\% of the total heterogeneity across studies can be accounted for by a dummy variable for type of experiment (0 = field, 1 = survey). This dummy variable has a significant association with the effectiveness of the information treatment at the 1\% significance level. In fact, with this dummy variable included, the overall estimate across studies is approximately 0, while the point estimate of the dummy variable is equal to -0.32.\footnote{Using a mixed effects model with a field experiment moderator.} This implies that the predicted treatment effect across experiments is not significantly different from zero when an indicator for type of experiment is included in the model. 

\section{Discussion} \label{sec: discussion}

What accounts for the large difference in treatment effects between field and survey experiments? Three possibilities are publication bias, social desirability bias, and the nature of the survey designs. Null results may be less likely to be published than significant results, particularly in a survey setting. Respondents in survey experiments may also behave in a normatively desirable manner according to the perceived norms of society and/or the researcher. It is also possible that more complex factorial designs - such as conjoint experiments - may more successfully approximate real-world settings, and by extension field experiments.  

\subsection{Publication bias and p-hacking}\label{sec: publication}

A quick look at the papers included in the meta-analysis shows that of the ten field experiments found, only six are published. By contrast, only three of the 18 survey experiment papers remain unpublished, and these are recent drafts. This may reflect that the null results that arise from field experiments are less likely to be published than their survey counterparts with large and highly significant negative treatment effects.

In order to more formally test for publication bias, I first attempt to employ the p-curve developed in \citet{simonsohn2014p1, simonsohn2014p2} and \citet{simonsohn2015better}. The p-curve is based on the premise that only ``significant'' results are typically published, and depicts the distribution of statistically significant p-values for a set of published studies. The shape of the p-curve is indicative of whether or not the results of a set of studies are derived from true effects, or from p-hacking. If effect sizes are clustered around 0.05 (i.e. the p-curve is ``left skewed''), this may be evidence of p-hacking, indicating that studies with p-values just below 0.05 are ``selectively reported.'' If the p-curve is ``right skewed'' and there are more low p-values (0.01), this is evidence of true effects.  

All significant survey experimental results included in the meta-analysis are significant at the 1\% level (making construction of a ``curve'' with bins of width 0.01 impossible), implying that publication bias likely does not explain the large negative treatment effects in survey experiments. Instead, it appears that the difference in experimental design itself accounts for the difference in the magnitude of treatment effects in field versus survey experiments. For field experiments, there is not a large enough number of published experiments to make the p-curve viable. Only six studies are published, and of these only four are significant at at least the 5\% level.

Next, I test for publication bias by examining funnel plot asymmetry. A funnel plot depicts the outcomes from each study on the x-axis and their corresponding standard errors on the y-axis. The chart is overlaid with an inverted triangular confidence interval region (i.e. the ``funnel"), which should contain 95\% of the studies if there is no bias or between study heterogeneity. If studies with insignificant results remain unpublished or there is a large degree of asymmetry, the funnel plot may be asymmetric. Both visual inspection and regression tests of funnel plot asymmetry reveal an asymmetric funnel plot when survey and field experiments are grouped together (see \autoref{fig: funnel_re_all} and \autoref{tab: funnel}). However, this asymmetry disappears when accounting for heterogeneity by type of experiment, either with the inclusion of a field experiment moderator (dummy) variable or by analyzing field and survey experiments separately (see \autoref{fig: funnel_all_mod}, \autoref{fig: funnel_re_field}, \autoref{fig: funnel_re_survey}, and \autoref{tab: funnel}). Once again, this implies that differences in the experimental design likely account for the difference in the magnitude of treatment effects in field versus survey experiments, not publication bias.


\subsection{Social desirability bias}\label{sec: sdb}

A second possible explanation is social desirability bias, in which survey respondents under-report behavior that they believe to be socially undesirable. The respondent may perceive a particular response to be normatively desirable by society as whole, by the researcher(s) conducting the experiment, or both, and respond to the survey in accordance with that norm. In the case of corruption, respondents are likely to perceive corruption as both normatively ``wrong,'' as well as harmful to society, the economy, and their own personal well-being.\footnote{Non-experimental surveys indicate the respondents in highly corrupt countries tend to view corruption as a serious problem that often tops their list of political considerations.} In a hypothetical vignette, they may therefore choose the socially desirable option (no corruption), particularly when the respondent is aware that he or she is being observed by a researcher. 

A related explanation may be the selection of the socially desirable option when there are few downsides to doing so. A hypothetical vignette has virtually no costs to selecting the socially desirable option, even when moderating variables are included. In a field experiment, however, the cost of changing one's vote may be higher. Voters may have pre-existing opinions of real candidates that make them discount corruption information, or may have strong material and/or ideological incentives to stick with their candidate. 

How might we overcome social desirability bias in survey experiments? One option is to eschew hypothetical candidates in favor of real candidates in experiments conducted during the timing of actual elections. Of course, for ethical reasons this likely limits researchers to having actual information regarding the corrupt actions of candidates. A second option is the use of list experiments or experiments which ask about the expected behavior of other individuals in response to new information. List experiments are surprisingly uncommon in corruption experiments (none of the survey experiments included here use this method), but a vote buying\footnote{Typically considered a form of corruption.} experiment in Nicaragua estimated that only 2\% of respondents admitted directly to being offered compensation in exchange for their vote, but 24\% of respondents admitted to the practice in a list experiment \citep{gonzalez2012vote}. A third option, which I turn to next, is the use of more complex factorial designs such as conjoint experiments. 

\subsection{Survey complexity and conjoint experiments}\label{sec: conjoint}

As noted in \autoref{sec: moderators} above, the fact that moderating variables may dampen the salience of corruption to voters has not been lost on previous researchers, who have attempted to capture these factors via the inclusion of multiple treatment arms that vary factors such as policy stances, quality of information, economic benefit, and partisanship. However, the meta-analysis shown above indicates that even the inclusion of these moderators does not move point estimates close to the (approximately zero) field setting, in which all of these moderating factors may be salient to the voter. By contrast, conjoint experiments allow researchers to randomize a much larger host of candidate characteristics and may help illuminate the mechanisms that lead to these small effect sizes. This may also minimize social desirability bias, as it reduces the probability that the respondent is aware of the researcher's primary experimental manipulation of interest (e.g. corruption).\footnote{This is explicitly mentioned by \citet{hainmueller2014causal}, who argue that conjoint experiments give respondents ``various attributes and thus [they] can often find multiple justifications for a given choice.'' Note, however, that an experiment does not necessarily need to be a conjoint design to have this feature. Conjoint experiments encourage researchers to randomize more attributes and therefore typically contain more complex hypothetical vignettes. However, the same vignette complexity could be achieved without full randomization of these attributes. } 

Researchers have thus far tended to present the results of conjoint experiments as individual average marginal component effects (AMCEs), then compare the magnitude of these effect sizes. A more appropriate method in some cases may be to calculate average marginal effects in order to present predicted probabilities of voting for a candidate with contextually plausable properties.\footnote{This method is utilized by \citet{teele2018ties} to examine the probability of voting female or male candidates holding other candidate attributes constant, and is discussed in more detail by \citet{leeper2018measuring}.} For example, in a conjoint experiment including corruption information, this might be interpreted as the probability of voting for a candidate that is both corrupt and possesses other particular feature levels (e.g. party membership or policy positions), marginalizing across all other features in the experiment.

To illustrate this point, I replicate the conjoint experiment conducted in Italy by \citet{franchino2015voting}\footnote{To my knowledge, this remains the only published conjoint experiment with a corruption treatment and publicly available replication data.} and present both AMCEs and predicted probabilities.\footnote{Note that I group the ``investigated for corruption'' and ``convicted of corruption'' levels into a single ``corrupt'' attribute in my replications.} The predicted probabilities are presented as a function of corruption and two policy positions - tax policy and same sex marriage - for conservative and liberal respondents. The charts therefore show the probability of preferring a profile that has particular levels of tax policy, same sex marriage policy, and corruption, marginalizing across all other features in the experiment. Note that the authors correctly conclude that their typical ``respondent prefers a corrupt but socially and economically progressive candidate to a clean but conservative one.'' While I therefore illustrate how predicted probabilities can be used to draw conclusions that may be masked by examination of AMCEs alone, the authors themselves do not make this mistake. 

A casual interpretation of the traditional AMCE plot presented in \autoref{fig: fz_amce} suggests that it is very unlikely a corrupt candidate would be chosen by a respondent. By contrast, the predicted probabilities plots presented in \autoref{fig: fz_margins_right} and \autoref{fig: fz_margins_left} show that even for corrupt candidates in the conjoint, the right policy platform can garner over 50\% of the predicted hypothetical vote. 

\begin{figure}[!htb]
\includegraphics{../figs/fz_amce.pdf}
\caption{\citet{franchino2015voting} conjoint: average marginal component effects}
\label{fig: fz_amce}
\end{figure}


\begin{figure}[!htb]
\includegraphics{../figs/fz_margins_right.pdf}
\vspace{0.2cm}
\caption{\citet{franchino2015voting} conjoint: can policy positions overcome corruption (conservative respondents)?}
\small
\vspace{-0.3cm}
\label{fig: fz_margins_right}

\vspace{1cm}

\includegraphics{../figs/fz_margins_left.pdf}
\vspace{0.2cm}
\caption{\citet{franchino2015voting} conjoint: can policy positions overcome corruption (liberal respondents)?}
\small
\vspace{-0.3cm}
\label{fig: fz_margins_left}
\end{figure}

Policy profiles that result in over 50\% of voters selecting a ``corrupt'' candidate may not be outliers in real-world scenarios. Unlike in conjoint experiments, real-world candidates' policy profiles are not selected randomly, but rather represent choices designed to appeal to voters. It may therefore be preferable to analyze conjoint experiments as above, comparing outlier characteristics (e.g. corruption) to realistic candidate profiles rather than fully randomized candidate profiles. For example, in the US context, perhaps the relevant metric of interest would be to look at the impact of corruption on vote choice for a Democratic respondent examining a Democratic candidate who espouses their preferred policy positions and attributes, rather than looking at the magnitude of the corruption AMCE versus each individual policy AMCE. 

\section{Conclusion} \label{sec: conclusion}

Competitive elections should naturally create a system of accountability, whereby voters expel politicians from office for engaging in malfeasance. However, this mechanism cannot operate without informed voters who are aware of the actions of politicians. In an effort to test whether voters adequately hold politicians accountable for malfeasance, political scientists and economists have turned to experimental methods to test the causal effect of learning about politician corruption on vote choice.

A meta-analytic assessment of these experiments reveals that the conclusions drawn differ drastically depending on whether the experiment was deployed in the field and monitored actual vote choice, versus hypothetical vote choice in an online setting. Across field experiments, the aggregate treatment effect of providing information about corruption on vote share is approximately zero. By contrast, in survey experiments corrupt candidates are punished by respondents by approximately 33-35 percentage points, depending on estimation methods. 

I explore four possible explanations that may explain this discrepancy: publication bias and/or p-hacking, social desirability bias, lack of complexity and realism of hypothetical vignettes, and misinterpretation of results from conjoint experiments. I find no evidence of publication bias, implying that the discrepancy arises due to differences in experimental design. Past experiments have suggested that social desirability bias may arise in corruption experiments, suggesting that survey experiments may be capturing strong anti-corruption norms rather than realistic voter behavior. These same norms may cause voters to select the clean candidate in a simple hypothetical vignette where few candidate traits are known. High-dimension factorial designs such as conjoint experiments may alleviate this issue. However, it may be preferable to analyze vote-choice conjoint experiments by comparing the probability of voting for a candidate with outlier characteristics such as corruption to the probability of voting for a realistic candidate without this characteristic, rather than examining differences in average marginal component effects across fully randomized candidate profiles, since these are untenable and so tell us little about real-world electoral choices. 

These findings suggest that while vote-choice survey experiments may provide information on the directionality of informational treatments in idealized hypothetical scenarios, the point estimates they provide may not be representative of real-world voting behavior. More generally, researchers should exercise caution when interpreting actions taken in hypothetical vignettes as indicative of real world behavior such as voting. 


\clearpage
\pagebreak

\pdfbookmark[1]{References}{References}
\bibliography{bibliography}

\pagebreak

\appendix
\setcounter{table}{0}
\setcounter{figure}{0}
\renewcommand\thetable{\Alph{section}.\arabic{table}}
\renewcommand\thefigure{\Alph{section}.\arabic{figure}}
\section{Appendix} \label{Appendix}

\subsection{Lab experiments}

\begin{table}[!htbp] \centering 
  \caption{Lab experiments}
  \label{tab:lab} 
  \small
\begin{tabular}{@{\extracolsep{5pt}} cccccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 Study & Country & ATE \\ 
 \hline \\[-1.8ex] 
\citet{arvate2017condemning} & Brazil & Negative \\
\citet{azfar2007transparency} & USA & Negative \\
\citet{rundquist1977corrupt}\textsuperscript{1} & USA & Negative \\
\citet{solaz2018group} & UK & Negative \\
\hline \\[-1.8ex] 
\end{tabular} 
 \begin{tablenotes}
\footnotesize
\item \textsuperscript{1} The candidate is always corrupt in the \citet{rundquist1977corrupt} experiment. A ``corruption'' point estimate is therefore not provided in the coefficient plot below.
    \end{tablenotes}
\end{table} 
\FloatBarrier

\vspace{2cm}

\begin{figure}[!htb]
\includegraphics[scale=0.93]{../figs/lab.pdf}
\vspace{0.2cm}
\caption{Lab experiments: Average treatment effect of corruption information on vote share}.
\small
\vspace{-0.3cm}
\label{fig: lab}
\end{figure}

\pagebreak
\subsection{Information quality}

\begin{figure}[!htb]
\includegraphics[scale=0.93]{../figs/quality.pdf}
\vspace{0.2cm}
\caption{Survey experiments by information quality: Average treatment effect of corruption information on vote share}.
\small
\vspace{-0.3cm}
\label{fig: quality}
\end{figure}


\pagebreak
\subsection{Robustness checks}

\begin{figure}[!htb]
\includegraphics[scale=0.93]{../figs/field_no_banerjee.pdf}
\vspace{0.2cm}
\caption{Field experiments: Average treatment effect of corruption information on incumbent vote share (excluding \cite{banerjee2010can} and \citet{banerjee2011informed})}.
\small
\vspace{-0.3cm}
\label{fig: meta-field_no_banerjee}

\includegraphics[scale=0.93]{../figs/survey_defig.pdf}
\vspace{0.2cm}
\caption{Survey experiments: Average treatment effect of corruption information on incumbent vote share (including \cite{de2011voters})}.
\small
\vspace{-0.3cm}
\label{fig: meta-field_defig}
\end{figure}

\pagebreak

\subsection{Publication bias}

\vspace{-.8cm}

\begin{figure}[!htb]
\begin{centering}
\includegraphics[scale=0.54]{../figs/funnel_re_all.pdf}
\vspace{0.2cm}
\caption{Funnel plot: all experiments}.
\small
\vspace{-0.3cm}
\label{fig: funnel_re_all}
\end{centering}
\end{figure}

\vspace{-1.3cm}

\begin{figure}[!htb]
\begin{centering}
\includegraphics[scale=0.54]{../figs/funnel_all_mod.pdf}
\vspace{0.2cm}
\caption{Funnel plot: all experiments with field experiment moderator}.
\small
\vspace{-0.3cm}
\label{fig: funnel_all_mod}
\end{centering}
\end{figure}

\begin{figure}[!htb]
\begin{centering}

\includegraphics[scale=0.54]{../figs/funnel_re_field.pdf}
\vspace{0.2cm}
\caption{Funnel plot: field experiments}.
\small
\vspace{-0.3cm}
\label{fig: funnel_re_field}
\end{centering}
\end{figure}

\begin{figure}[!htb]
\begin{centering}
\includegraphics[scale=0.54]{../figs/funnel_re_survey.pdf}
\vspace{0.2cm}
\caption{Funnel plot: survey experiments}.
\small
\vspace{-0.3cm}
\label{fig: funnel_re_survey}
\end{centering}
\end{figure}

\begin{table}[ht] \centering 
  \caption{Regression tests for funnel plot asymmetry}
  \label{tab: funnel} 
  \small
  \vspace{0.5cm}
\begin{tabular}{@{\extracolsep{88pt}} cccccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 Studies included & p-value \\ 
 \hline \\[-1.8ex] 
All & 0.0016  \\
All with moderator & 0.4512  \\
Field &  0.8403  \\
Survey & 0.3159  \\
\hline \\[-1.8ex] 
\vspace{128in}
\end{tabular} 
\end{table} 
\FloatBarrier



\end{document} 